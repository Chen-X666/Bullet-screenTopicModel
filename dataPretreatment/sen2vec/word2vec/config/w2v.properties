#一些经验
#架构（sg）：skip-gram（慢、对罕见字有利）vs CBOW（快）
#训练算法（hs）：分层softmax（对罕见字有利）vs 负采样（对常见词和低纬向量有利）
#欠采样频繁词（sample）：可以提高结果的准确性和速度（适用范围1e-3到1e-5）
#文本大小（window）：skip-gram通常在10附近，CBOW通常在5附近
#大语料下，建议提高min_count，减少iter
#内存占用大约公式：词汇数*8*size/1000/1000/1000(GB)
#硬盘占用大约公式：词汇数*8/1000/1000/1000(GB)(实际上考虑到其模型的其他文件，最好再*10的大小)

# 训练算法，0为CBOW算法，1为skip-gram算法，默认为0
sg=1
# 特征向量的维度
size=300
# 词窗大小
window=5
# 最小词频
min_count=5
# 初始学习速率
alpha=0.025
# 0为负采样，1为softmax，默认为0
hs=1
#迭代次数
iter=10